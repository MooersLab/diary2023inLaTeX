\newglossaryentry{adaptive design}{name=\textbf{adaptive design},description={See Hybrid Monte Carlo.}}
\newglossaryentry{Approximate Bayesian Computation}{name=\textbf{Approximate Bayesian Computation},description={A class of computational methods rooted in Bayesian statistics. ABC methods bypass the evaluation of the likelihood function and widen the realm of models for which statistical inference can be considered. ABC methods make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection.}}
\newglossaryentry{autocorrelation time}{name=\textbf{autocorrelation time},description={The number of observations from an MCMC simulation that is equivalent to a single independent observation. It is also called the correlation length, and is often represented by  $\tau$. Estimates based on a MCMC simulation with a correlation time of $\tau$ will have standard errors that are about the same as those from an independent sample that is smaller in size by a factor of  $\tau$.}}
\newglossaryentry{candidate kernel}{name=\textbf{candidate kernel},description={The Metropolis-Hastings algorithm associated with a target density $\pi$ requires the choice a proposal kernel or candidate kernel or a conditional density from the transition of the Markov chain, $X^(t)$, from time, $t$, and its value, $X^(t+1)$,  at time $(t+1)$.}}
\newglossaryentry{censored data}{name=\textbf{censored data},description={Censoring hides values from points that are too large, too small, or both. The number of data points that were censored is known, unlike the case for truncated data. Data are right-censored if the value is greater than a threshold. The data are left-censored if the value is below a threshold. The censored data can be treated as missing data. In Stan, the censored data have their own array and their mean and sigma are sampled.}}
\newglossaryentry{correlation length}{name=\textbf{correlation length},description={See autocorrelation time.}}
\newglossaryentry{curse of dimensionality}{name=\textbf{curse of dimensionality},description={The various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions). These phenomena do not occur in the analysis of  3-D data.  The problem is caused by the exponential increase in volume associated with adding extra dimensions to a (mathematical) space. The term was coined by the applied mathematician Richard Ernest Bellman (August 26, 1920 – March 19, 1984) who introduced dynamic programming in 1953.}} 
\newglossaryentry{diminishing adaptation condition}{name=\textbf{diminishing adaptation condition},description={The distance between two consecutive Markov kernels must uniformly decrease to zero.}}
\newglossaryentry{effective sample size}{name=\textbf{effective sample size},description={With regards to MCMC, this is the correction factor, $\tau_T$, such that $\sigma^2_T / \tau_T$ is the variance of the empirical average.}} 
\newglossaryentry{equidispersion}{name=\textbf{equidispersion},description={Equal dispersion of parameter values.}}
\newglossaryentry{extra-dispersion}{name=\textbf{extra-dispersion},description={Extra dispersion.}}
\newglossaryentry{Hybrid Monte Carlo}{name=\textbf{Hybrid Monte Carlo},description={Also known as Hamiltonian Monte Carlo. A Markov chain Monte Carlo method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). It differs from the Metropolis–Hastings algorithm by reducing the correlation between successive sampled states by using a Hamiltonian evolution between states and additionally by targeting states with a higher acceptance criteria than the observed probability distribution. This causes it to converge more quickly to the absolute probability distribution. It was devised by Simon Duane, A.D. Kennedy, Brian Pendleton and Duncan Roweth in 1987.}}
\newglossaryentry{Hamiltonian Monte Carlo}{name=\textbf{Hamiltonian Monte Carlo},description={See Hybrid Monte Carlo.}}
\newglossaryentry{irreducibility}{name=\textbf{irreducibility},description={When a Markov chain has a stationary distribution and can eventually reach any region of the state space, no matter the initial value.}}
\newglossaryentry{leapfrog approximation}{name=\textbf{leapfrog approximation},description={The Metropolis-Hastings correction required by the Hamiltonian Monte Carlo.}}
%\newglossaryentry{Limited-memory Broyden–Fletcher–Goldfarb–Shanno}{name=\textbf{Limited-memory Broyden–Fletcher–Goldfarb–Shanno},description={The L-BFGS is an optimization algorithm in the family of quasi-Newton methods. It approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning. The L-BFGS estimates the inverse Hessian matrix to steer its search through variable space. On the other hand, the BFGS stores a dense n×n approximation to the inverse Hessian, where n is the number of variables in the problem. L-BFGS stores only a few vectors that represent the approximation implicitly.}}
\newglossaryentry{Markov Chain Monte Carlo}{name=\textbf{Markov Chain Monte Carlo},description={A class of algorithms that simulates a Markov chain whose stationary distribution is the target distribution of interest. The stationary chain generates a sample from the target distribution.}}
\newglossaryentry{Metropolis-within-Gibbs algorithm}{name=\textbf{Metropolis-within-Gibbs algorithm},description={This alogorithm aims to simulate a multidimensional distribution by successively simulating from some of the associated conditional distributions (the Gibbs part) and by using one Metropolis-Hastings steps.}}
\newglossaryentry{No U-turn sampler}{name=\textbf{No U-turn sampler},description={An adaptive algorithm that aims to find the best parameter settings by tracking the sample path and preventing HMC from retracing its steps in this path.}}
\newglossaryentry{overdispersion}{name=\textbf{overdispersion},description={When the observed variance is greater than the mean in count data.}}
\newglossaryentry{Poisson overdispersion}{name=\textbf{Poisson overdispersion},description={The Poisson distribution has a mean that is equal to its variance. When the observed variance is greater than the mean; this is known as overdispersion and indicates that the Poisson model is not appropriate. A common reason for overdispersikon ais the omission of relevant explanatory variables, or dependent observations. Under some circumstances, the problem of overdispersion can be solved by using quasi-likelihood estimation or a negative binomial distribution instead.}}
\newglossaryentry{Riemannian manifold Hamiltonian Monte Carlo}{name=\textbf{Riemannian manifold Hamiltonian Monte Carlo},description={aims to find the best parameter settings by providing adaptations using Riemannian geometry.}}
\newglossaryentry{simultaneous statistical inference}{name=\textbf{simultaneous statistical inference},description={See Hybrid Monte Carlo.}}
\newglossaryentry{'shock' nucleation}{name=\textbf{'shock' nucleation},description={Crystal nucleation as a result of the contact of highly concentrated protein stock solution with precipitant stock solution before mixing is complete. Shock nucleation causes the formation of crystal nuclei in solutions that are really in the metastable region of the phase diagram. }}
